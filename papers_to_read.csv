Name,URL,Description
"RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",http://arxiv.org/abs/2401.08406v1,"This paper discusses the pros and cons of two common approaches for incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. The authors propose a pipeline for fine-tuning and RAG, and present the tradeoffs for multiple popular LLMs. The paper focuses on a case study in the agricultural industry, demonstrating the effectiveness of the dataset generation pipeline and the benefits of RAG and fine-tuning in providing location-specific insights to farmers."
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine,http://arxiv.org/abs/2401.08396v1,"This paper titled 'Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine' discusses the evaluation of Generative Pre-trained Transformer 4 with Vision (GPT-4V) in medical challenge tasks. The paper extends the previous evaluations by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges. The evaluation results show that GPT-4V outperforms human physicians in multi-choice accuracy and also performs well in cases where physicians incorrectly answer. However, the paper highlights the occurrence of flawed rationales by GPT-4V in cases where it makes the correct final choices. The findings emphasize the need for further evaluations of GPT-4V's rationales before integrating such models into clinical workflows."
DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models,http://arxiv.org/abs/2401.08392v1,"The paper discusses DoraemonGPT, a system driven by large language models (LLMs) to handle dynamic video tasks. It focuses on understanding the dynamic nature of real-world scenarios by converting videos into a symbolic memory and using sub-task tools for spatial-temporal querying and reasoning. The paper also incorporates plug-and-play tools to assess external knowledge and introduces a novel LLM-driven planner for efficiently exploring the planning space. The system, DoraemonGPT, is evaluated and demonstrated to handle more complex questions in dynamic scenes."
Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models,http://arxiv.org/abs/2401.08350v1,"This paper discusses the challenges of machine translation in the context of advanced Large Language Models (LLMs). It revisits the six core challenges that have influenced Neural Machine Translation (NMT) progress and explores their ongoing relevance. The paper presents empirical findings that indicate LLMs effectively lessen the reliance on parallel data and enhance the translation of long sentences. However, challenges related to domain mismatch and rare word prediction still persist. The authors also identify three new challenges for LLMs in translation tasks. The paper provides datasets and models that are released at the given URL."
RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning,http://arxiv.org/abs/2401.08326v1,"The paper introduces RoTBench, a multi-level benchmark for evaluating the robustness of Large Language Models (LLMs) in tool learning. It discusses the current research gap in understanding LLMs' stability when faced with real-world noise and proposes RoTTuning, a strategy to enhance the robustness of LLMs in tool learning."
Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening,http://arxiv.org/abs/2401.08315v1,"This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. It utilizes LLM agents for decision-making, determining which candidates receive job offers or interviews. The results demonstrate that the automated resume screening framework is significantly faster than traditional methods and achieves improved performance in resume sentence classification and summarization compared to baseline models."
Anchor function: a type of benchmark functions for studying language models,http://arxiv.org/abs/2401.08309v1,"The paper proposes the concept of an anchor function, which is a benchmark function designed for studying language models. It demonstrates the utility of the anchor function with an example, revealing two basic operations by attention structures in language models: shifting tokens and broadcasting one token from one position to many positions. The anchor function framework opens up valuable research questions for further exploration in large language models."
DAPT: A Dual Attention Framework for Parameter-Efficient Continual Learning of Large Language Models,http://arxiv.org/abs/2401.08295v1,"The paper proposes a novel Dual Attention Framework called DAPT for parameter-efficient continual learning of large language models. It addresses the challenges of catastrophic forgetting and knowledge transfer by aligning the PET learning and selection modules. The paper demonstrates the superiority of DAPT in resisting catastrophic forgetting and facilitating knowledge transfer through extensive experiments on two continual learning benchmarks. Furthermore, DAPT exhibits scalability to different model sizes and unseen tasks."
Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models,http://arxiv.org/abs/2401.08294v1,"The paper presents Inferflow, an efficient and highly configurable inference engine for large language models (LLMs). It allows users to serve most common transformer models by modifying configuration files, without writing source code. Inferflow implements a modular framework and introduces 3.5-bit quantization and hybrid model partitioning for multi-GPU inference. The paper is a technical report of Inferflow."
AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception,http://arxiv.org/abs/2401.08276v1,"This paper proposes AesBench, an expert benchmark to evaluate the aesthetic perception capacities of multimodal large language models (MLLMs) on image aesthetics perception. It introduces an Expert-labeled Aesthetics Perception Database (EAPD) and a set of integrative criteria to measure the aesthetic perception abilities of MLLMs. The experimental results show that current MLLMs only possess rudimentary aesthetic perception ability."
Large Language Models are Null-Shot Learners,http://arxiv.org/abs/2401.08273v1,"This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the 'Examples' section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across LLMs also potentially indicate a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results."
Generative Multi-Modal Knowledge Retrieval with Large Language Models,http://arxiv.org/abs/2401.08206v1,"This paper proposes an innovative end-to-end generative framework for multi-modal knowledge retrieval, taking advantage of large language models (LLMs) to effectively serve as virtual knowledge bases. The paper introduces an object-aware prefix-tuning technique to guide multi-grained visual learning and aligns multi-grained visual features into the textual feature space of the LLM. It also constructs instruction data with a unified format for model training and proposes a knowledge-guided generation strategy to impose prior constraints in the decoding steps. The experiments conducted on three benchmarks show significant improvements compared to strong baselines."
MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline,http://arxiv.org/abs/2401.08190v1,"The paper discusses the challenges in modeling mathematical reasoning using large language models (LLMs) and proposes a novel math dataset enriched with a Python code interpreter. It also introduces a fine-tuning protocol for math-specific LLMs, which has shown significant improvement in performance. The authors have made the model checkpoints and dataset publicly available."
PRewrite: Prompt Rewriting with Reinforcement Learning,http://arxiv.org/abs/2401.08189v1,"The paper titled 'PRewrite: Prompt Rewriting with Reinforcement Learning' discusses the development of an automated tool called PRewrite for prompt engineering. It leverages the Reinforcement Learning (RL) framework to optimize and rewrite initial prompts, generating highly effective new prompts. The study conducted experiments on diverse datasets and found that the prompts generated with PRewrite outperformed professionally crafted prompts and prompts generated using other methods. The paper is relevant to the topic of Large Language Models as it focuses on improving prompt engineering for LLM-based applications."
A Study on Training and Developing Large Language Models for Behavior Tree Generation,http://arxiv.org/abs/2401.08089v1,"The paper presents an exploration of the application potential of large language models (LLM) in generating behavior trees (BTs) for complex tasks. It proposes a novel methodology that leverages LLMs' robust representation and reasoning abilities to overcome challenges related to task complexity, model adaptability, and reliability. The paper introduces a BT generation framework based on LLMs, trains the BT generation model using synthetic data, and emphasizes data verification to ensure the effectiveness and execuability of the generated BTs. It also explores agent design and development schemes with LLMs. Overall, the paper provides a reference for researchers interested in BT generation based on LLMs."
Enhancing Document-level Translation of Large Language Model via Translation Mixed-instructions,http://arxiv.org/abs/2401.08088v1,"This paper discusses the challenge of document-level translation using large language models (LLMs). It proposes an approach that combines sentence-level and document-level instructions to fine-tune LLMs, enabling consistent translation performance from the sentence level to documents containing as many as 2048 tokens. The experimental results show that this approach significantly enhances the document-level translation capabilities of LLMs on 10 language pairs, improving translation quality and discourse coherence."
Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis,http://arxiv.org/abs/2401.08046v1,This paper presents a comprehensive analysis of the impact of prompts on the text generated by large language models (LLMs) and highlights the potential lack of robustness in one of the current state-of-the-art GPT detectors. The authors propose a reference-based Siamese detector named Synthetic-Siamese to address the lack of robustness of previous detectors and significantly improve the baseline performances in realistic academic writing scenarios.
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,http://arxiv.org/abs/2401.08026v1,"The paper presents JustiLM, a novel few-shot Justification generation based on retrieval-augmented Language Model. It proposes a realistic approach to generate justification for fact-checking by using retrieved evidence. The paper introduces a new benchmark dataset called ExClaim for explainable fact-checking of real-world claims. The experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines and can also enhance veracity classification."
Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination,http://arxiv.org/abs/2401.08025v1,"The paper proposes a method called Self-Imagine, which leverages a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. The approach does not require any additional training data or training. The paper evaluates the approach in three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art VLM and shows an improvement in performance on math tasks (+4.62% to +9.30%) and most general-purpose reasoning tasks."
A Novel Approach for Automatic Program Repair using Round-Trip Translation with Large Language Models,http://arxiv.org/abs/2401.07994v1,"This paper investigates the use of Large Language Models (LLMs) in Automatic Program Repair (APR). It proposes using Round-Trip Translation (RTT) with LLMs to correct bugs in code. The paper tests this approach using eight recent LLMs pre-trained on code and four common program repair benchmarks in Java. The findings show that RTT with LLMs is able to repair a significant number of bugs, including unique bugs that are not repaired by other LLMs fine-tuned for APR. This highlights the viability of round-trip translation with LLMs as a technique for automated program repair and its potential for research in software engineering."
Leveraging External Knowledge Resources to Enable Domain-Specific Comprehension,http://arxiv.org/abs/2401.07977v1,"The paper discusses the problem of domain shift in language models and proposes a method using Multi-Layer Perceptrons (MLPs) to align and integrate embeddings from knowledge graphs with pre-trained language models (LMs). The authors show that this approach improves the performance of general-purpose transformers in domain-specific applications, specifically in span detection and multiple-choice question tasks. The paper is related to the topic of Large Language Models because it demonstrates a technique to enhance the capabilities of such models in understanding domain-specific text."
AI-as-exploration: Navigating intelligence space,http://arxiv.org/abs/2401.07964v1,"This paper discusses the value of AI-as-exploration by focusing on a specific case study, which is recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. It highlights that Large Language Models, despite showing human-level accuracy in such a task, solve it in ways radically different, but no less relevant to intelligence research, to those hypothesized for humans."
A Study on Large Language Models' Limitations in Multiple-Choice Question Answering,http://arxiv.org/abs/2401.07955v1,"This paper analyzes the capabilities and limitations of 26 small open-source models in the task of multiple-choice question answering. It highlights that 65% of the models do not understand the task properly, only 4 models can properly select an answer from the given choices, and only 5 models are choice order independent. The findings are alarming considering the extensive use of multiple-choice tests with these models. The paper recommends exercising caution and testing task understanding before using multiple-choice questions to evaluate large language models in any field."
SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning,http://arxiv.org/abs/2401.07950v1,"This paper introduces SciGLM, a suite of scientific language models that address the limitations of Large Language Models (LLMs) in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. The paper describes a novel self-reflective instruction annotation framework used to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. The paper also presents the fine-tuning of the ChatGLM family of language models with a curated dataset called SciInstruct, which enhances their capabilities in scientific and mathematical reasoning. The results show consistent improvements in both the base model and larger-scale models without sacrificing the language understanding capabilities of the base model. The paper provides the SciInstruct dataset, SciGLM, alongside a self-reflective framework, and fine-tuning code for the wider research community."
SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT,http://arxiv.org/abs/2401.07944v1,"This paper uses the BERT model, which is a transformer-based architecture, to solve task 4A, English Language, Sentiment Analysis in Twitter of SemEval2017. BERT is a very powerful large language model for classification tasks when the amount of training data is small. For this experiment, we have used the BERT BASE model, which has 12 hidden layers. This model provides better accuracy, precision, recall, and f1 score than the Naive Bayes baseline model. It performs better in binary classification subtasks than the multi-class classification subtasks. We also considered all kinds of ethical issues during this experiment, as Twitter data contains personal and sensible information. The dataset and code used in our experiment can be found in this GitHub repository."
Can Large Language Models Explain Themselves?,http://arxiv.org/abs/2401.07927v1,"The paper explores the interpretability-faithfulness of self-explanations in large language models (LLMs), which are directly accessible to the public. It proposes employing self-consistency checks as a measure of faithfulness in LLM's self-explanations and applies them to three types of self-explanations: counterfactuals, importance measures, and redactions. The findings show that faithfulness is both task and model dependent."
Word Boundary Information Isn't Useful for Encoder Language Models,http://arxiv.org/abs/2401.07923v1,"The paper explores whether word boundary information is useful for transformer-based language models. Through an extensive experimental setup, including the pre-training of 29 models, the authors find no substantial improvements from alternative approaches to including word boundary information. This suggests that modifying tokenisers to remove word boundary information does not result in a loss of useful information."
The Pitfalls of Defining Hallucination,http://arxiv.org/abs/2401.07897v1,The paper examines current classifications of hallucination and omission in Data-text NLG and proposes a logic-based synthesis of these classifications. It discusses the limitations of current thinking about hallucination and implications for Large Language Models (LLMs).
Learned Best-Effort LLM Serving,http://arxiv.org/abs/2401.07886v1,The paper titled 'Learned Best-Effort LLM Serving' is related to the topic of Large Language Models (LLMs). It presents a best-effort serving system that employs deep reinforcement learning to adjust service quality based on the task distribution and system load. The system can maintain availability with over 10x higher client request rates and serves above 96% of peak performance more often than static serving on unpredictable workloads. The paper argues that learned best-effort LLM serving is applicable in a wide variety of settings and provides application developers great flexibility to meet their specific needs.
"The Chronicles of RAG: The Retriever, the Chunk and the Generator",http://arxiv.org/abs/2401.07883v1,"This paper discusses the implementation, optimization, and evaluation of Retrieval Augmented Generation (RAG) for the Brazilian Portuguese language. It explores various methods to answer questions about the first Harry Potter book using OpenAI's gpt-4, gpt-4-1106-preview, gpt-3.5-turbo-1106, and Google's Gemini Pro. The paper focuses on improving the quality of the retriever, achieving a 35.4% improvement of MRR@10 compared to the baseline. It also presents the complete architecture of RAG with recommendations. The paper falls within the topic of Large Language Models as it discusses the challenges and techniques related to enabling LLMs to access external data and generate coherent text."
"The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",http://arxiv.org/abs/2401.07872v1,"The paper is a detailed survey that delves into the multifaceted aspects of context length extension techniques in Large Language Models (LLMs). It discusses why context length extension is essential, presents an overview of existing strategies, and explores the challenges and evaluation standards in this domain. The paper aims to serve as a valuable resource for researchers in the field of Large Language Models."
